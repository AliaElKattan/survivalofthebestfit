<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no"><meta name="theme-color" content="#4DA5F4"><meta name="csrf-token"><title> Survival of the Best Fit</title><link rel="shortcut icon" href="favicon.png"><link rel="stylesheet" href="../vendor/bootstrap/css/bootstrap.min.css"><link rel="stylesheet" href="../vendor/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,700"><link rel="stylesheet" href="../css/styles-website.min.css"></head><body><body class="resources"><h1>Machines, Bias, and Fairness</h1><h4 class="header-box">HOW DOES MACHINE BIAS WORK?</h4><p>Bias has long been a problem in recruiting. Research has shown that ref1, ref2, and ref3. One might think that computers can help us eliminate this human bias, but automation is not always the answer. There are several ways bias can be propagate through computer code. Since ‘machine learning’ algorithms work by learning from previous trends, bias could exist if the learning data we give our program is already biased. For example, if you teach a computer to learn from the past century of successful CVs, it can pick on trends that reflect historical and societal biases. <br> <br> As a matter of fact, sometimes the data isn’t biased because it represents a historical reality, it’s also inaccurate. According to research on gender bias in natural language processing (i.e. computers understanding text), ref.</p><h4 class="header-box">HOW DO WE FIX IT?</h4><p>One of the main problems with using software to make important decisions is that you often can’t track down why a decision was made. In machine learning, this is what we call the “black box problem”. A software would learn from data and try to replicate it, but it does not let you know specifically what that decision-making process looks like. In cases where software solutions could provide corporations and governments from accountability, it may be a better option to refrain from using them at all. <br> <br> The solution to bias is not straightforward. It might be easy to think that to fix bias in the dataset, all we need to do is to gather a representational dataset. Since machine learning works on mass data, however, this may still not help minorities. Navigating bias and representation is a complex topic, and so is fairness. If we want to build equitable software systems that do not discriminate against or disadvantage any members of our society, we must open up this conversation to people who are not software engineers, or in the tech industry. </p></body><script src="../../game/assets/text/textTemplate.js"></script><!--script(async src="https://www.googletagmanager.com/gtag/js?id=UA-138331065-1")--><!--script.--><!--  window.dataLayer = window.dataLayer || []--><!--  function gtag(){dataLayer.push(arguments)}--><!--  gtag('js', new Date())--><!--  gtag('config', 'UA-138331065-1')--></body></html>